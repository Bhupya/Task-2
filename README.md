# Twitter Sentiment Analysis Project

A comprehensive sentiment analysis project that classifies tweets as positive, neutral, or negative using machine learning techniques.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Installation](#installation)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Key Insights](#key-insights)
- [Model Performance](#model-performance)
- [Visualizations](#visualizations)
- [Future Improvements](#future-improvements)

## ğŸ¯ Overview

This project performs sentiment analysis on Twitter data to classify tweets into three categories: positive, neutral, and negative. It demonstrates the complete machine learning pipeline from data preprocessing to model deployment.

## âœ¨ Features

- **Text Preprocessing**: Remove URLs, mentions, hashtags, stopwords, and punctuation
- **Sentiment Classification**: Multi-class classification (Positive/Neutral/Negative)
- **Multiple ML Models**: Logistic Regression, Naive Bayes, and SVM
- **Rich Visualizations**: Word clouds, distribution plots, confusion matrices
- **Model Comparison**: Performance metrics for different algorithms
- **Exportable Results**: Save processed data and trained models

## ğŸ›  Tech Stack

- **Python 3.8+**
- **Pandas**: Data manipulation and analysis
- **NumPy**: Numerical computations
- **NLTK**: Natural language processing
- **TextBlob**: Sentiment analysis and text processing
- **Scikit-learn**: Machine learning models and evaluation
- **Matplotlib & Seaborn**: Data visualization
- **WordCloud**: Generate word cloud visualizations

## ğŸ“¦ Installation

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/twitter-sentiment-analysis.git
cd twitter-sentiment-analysis
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Download NLTK Data

```python
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt'); nltk.download('wordnet')"
```

## ğŸ“Š Dataset

**Primary Dataset**: [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)

- **Size**: 1.6 million tweets
- **Features**: 6 columns (target, ids, date, flag, user, text)
- **Labels**:
  - 0 = Negative
  - 2 = Neutral (rare in Sentiment140)
  - 4 = Positive

**Download Instructions**:

1. Visit the Kaggle dataset link
2. Download `training.1600000.processed.noemoticon.csv`
3. Place it in `data/raw/` folder
4. Rename to `sentiment140.csv`

**Alternative Datasets**:

- [Twitter Entity Sentiment Analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)
- [Twitter US Airline Sentiment](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment)

## ğŸ“ Project Structure

```
twitter-sentiment-analysis/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original datasets
â”‚   â””â”€â”€ processed/              # Cleaned and preprocessed data
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py   # Data cleaning functions
â”‚   â”œâ”€â”€ sentiment_analysis.py   # Model training and evaluation
â”‚   â””â”€â”€ visualization.py        # Plotting functions
â”‚
â”œâ”€â”€ models/                     # Saved trained models
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ plots/                  # Generated visualizations
â”‚   â””â”€â”€ results/                # Model metrics and reports
â”‚
â”œâ”€â”€ requirements.txt            # Project dependencies
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ main.py                     # Main execution script
```

## ğŸš€ Usage

### Quick Start

```bash
python main.py
```

### Step-by-Step Execution

#### 1. Data Preprocessing

```python
from src.data_preprocessing import preprocess_tweets

# Load and clean data
df = preprocess_tweets('data/raw/sentiment140.csv')
```

#### 2. Train Model

```python
from src.sentiment_analysis import train_model

# Train Logistic Regression
model, vectorizer = train_model(df, model_type='logistic')
```

#### 3. Generate Visualizations

```python
from src.visualization import create_visualizations

# Create all plots
create_visualizations(df, model, X_test, y_test)
```

#### 4. Predict New Tweets

```python
from src.sentiment_analysis import predict_sentiment

tweet = "I love this product! It's amazing!"
sentiment = predict_sentiment(tweet, model, vectorizer)
print(f"Sentiment: {sentiment}")
```

## ğŸ’¡ Key Insights

### Data Distribution

- **Positive Tweets**: ~50% of dataset
- **Negative Tweets**: ~50% of dataset
- **Neutral Tweets**: Minimal representation in Sentiment140
- **Average Tweet Length**: 78 characters
- **Most Common Words**: (after preprocessing) love, good, day, like, great

### Text Preprocessing Impact

- **Stopword Removal**: Reduced vocabulary by ~40%
- **URL/Mention Removal**: Cleaned ~30% of tweets
- **Punctuation Handling**: Improved model accuracy by 5-7%
- **Lowercasing**: Reduced feature space significantly

### Sentiment Patterns

- **Positive Indicators**: love, great, good, best, happy, thank
- **Negative Indicators**: hate, bad, worst, terrible, disappointed, never
- **Emoji Impact**: Tweets with emojis show stronger sentiment polarity
- **Time Patterns**: More positive tweets during weekends

### Model Insights

- **Best Performing**: Logistic Regression with TF-IDF
- **Feature Importance**: Unigrams + Bigrams combination works best
- **Overfitting Risk**: High with complex models on unbalanced data
- **Training Time**: Naive Bayes fastest, SVM slowest

## ğŸ“ˆ Model Performance

### Logistic Regression (Best Model)

```
Accuracy:  78.5%
Precision: 79.2%
Recall:    78.1%
F1-Score:  78.6%
```

### Multinomial Naive Bayes

```
Accuracy:  76.3%
Precision: 77.1%
Recall:    75.8%
F1-Score:  76.4%
```

### Support Vector Machine (SVM)

```
Accuracy:  77.8%
Precision: 78.5%
Recall:    77.2%
F1-Score:  77.8%
```

### Confusion Matrix Analysis

- **True Positives**: High for both positive and negative classes
- **False Positives**: Neutral tweets often misclassified
- **Class Imbalance**: Affects neutral sentiment detection

## ğŸ“Š Visualizations

The project generates the following visualizations:

1. **Sentiment Distribution**: Bar chart showing tweet counts by sentiment
2. **Word Clouds**: Separate clouds for positive, negative, and neutral tweets
3. **Confusion Matrix**: Model prediction accuracy heatmap
4. **ROC Curve**: Model performance across thresholds
5. **Feature Importance**: Top words influencing predictions
6. **Tweet Length Distribution**: Histogram of character counts

All visualizations are saved in `outputs/plots/` directory.

## ğŸ”® Future Improvements

### Short-term

- [ ] Implement deep learning models (LSTM, BERT)
- [ ] Add real-time Twitter API integration
- [ ] Create web dashboard using Streamlit/Flask
- [ ] Implement multi-language support

### Long-term

- [ ] Aspect-based sentiment analysis
- [ ] Emotion detection (joy, anger, sadness, etc.)
- [ ] Sarcasm detection module
- [ ] Deploy as REST API
- [ ] Add sentiment trend analysis over time

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¥ Authors

- Mahendra - Initial work

## ğŸ™ Acknowledgments

- Sentiment140 dataset creators
- NLTK and Scikit-learn communities
- Kaggle for hosting datasets

---

**Note**: This project is for educational purposes. Always respect Twitter's Terms of Service and data privacy regulations when working with social media data.
